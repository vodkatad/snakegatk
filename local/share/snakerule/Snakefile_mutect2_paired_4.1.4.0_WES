def find_matched_normal(wildcards):
    return {'normal': ALIGN_DIR+"/realigned_"+find_matched_normal_sample(wildcards)+".sorted.bam"}
    

if PON != "":
    rule mutect:
        input: unpack(find_matched_normal), bam=ALIGN_DIR+"/realigned_{sample}.sorted.bam", gnomad=GNOMAD+"/af-only-gnomad.hg38.vcf.gz", bed=SEXONS, reference=REF_DIR+"/GRCh38.d1.vd1.fa", pon=PON
        output: vcf=protected(MUTECT_DIR+"/{sample}.vcf.gz"), stats=MUTECT_DIR+"/{sample}.vcf.gz.stats", f1r2=MUTECT_DIR+"/{sample}_f1r2.tar.gz"
        singularity: GATK_SING
        docker: GATK_ODOCKER
        log: MUTECT_DIR+"/{sample}.vcf.gz.log"
        params: padding=PADDING, cores=CORES, normal=find_matched_normal_sample
        shell: 
            """
                gatk Mutect2 --native-pair-hmm-threads {params.cores} --independent-mates -normal {params.normal} -R {input.reference} -I {input.normal} -I {input.bam} -O {output.vcf} --interval-padding {params.padding} --germline-resource {input.gnomad} -L {input.bed} --panel-of-normals {input.pon} --f1r2-tar-gz {output.f1r2}  2> {log}
            """
else:
    rule mutect:
        input: unpack(find_matched_normal), bam=ALIGN_DIR+"/realigned_{sample}.sorted.bam", gnomad=GNOMAD+"/af-only-gnomad.hg38.vcf.gz", bed=SEXONS, reference=REF_DIR+"/GRCh38.d1.vd1.fa"
        output: vcf=protected(MUTECT_DIR+"/{sample}.vcf.gz"), stats=MUTECT_DIR+"/{sample}.vcf.gz.stats", f1r2=MUTECT_DIR+"/{sample}_f1r2.tar.gz"
        singularity: GATK_SING
        docker: GATK_ODOCKER
        log: MUTECT_DIR+"/{sample}.vcf.gz.log"
        params: padding=PADDING, cores=CORES, normal=find_matched_normal_sample
        shell: 
            """
                gatk Mutect2 --native-pair-hmm-threads {params.cores} --independent-mates -normal {params.normal} -R {input.reference} -I {input.normal} -I {input.bam} -O {output.vcf} --interval-padding {params.padding} --germline-resource {input.gnomad} -L {input.bed} --f1r2-tar-gz {output.f1r2} 2> {log}
            """

rule getpileup:
    input: bam=ALIGN_DIR+"/realigned_{sample}.sorted.bam", gnomad=GNOMAD+"/gnomad.forcontamination.noalt.exomes.vcf", bed=SEXONS
    output: temp(MUTECT_DIR+"/{sample}.pileup.table")
    singularity: GATK_SING
    docker: GATK_ODOCKER
    params: padding=PADDING
    log: MUTECT_DIR+"/{sample}.pileup.table.log"
    shell: 
        """
            gatk GetPileupSummaries -I {input.bam} -V {input.gnomad} -L {input.bed} -O {output} --interval-padding {params.padding} 2> {log}
        """

# note: if mutect will ever be called in // on different chr we will need to put back together things here (multiple -I)
rule learnOrientationModel:
    input:  MUTECT_DIR+"/{sample}_f1r2.tar.gz"
    output: MUTECT_DIR+"/{sample}_read-orientation-model.tar.gz"
    singularity: GATK_SING
    docker: GATK_ODOCKER
    log: MUTECT_DIR+"/{sample}_read-orientation-model.tar.gz.log"
    shell:
        """
            gatk LearnReadOrientationModel -I {input} -O {output} 2> {log}
        """

def find_matched_normal_contable(wildcards):
    return {'normal': MUTECT_DIR+"/"+find_matched_normal_sample(wildcards)+".pileup.table"}
    

rule calculatecontamination:
    input: unpack(find_matched_normal_contable), tumor=MUTECT_DIR+"/{sample}.pileup.table"
    output: table=MUTECT_DIR+"/{sample}.contamination.table", seg=MUTECT_DIR+"/{sample}.tum.seg"
    singularity: GATK_SING
    docker: GATK_ODOCKER
    log: MUTECT_DIR+"/{sample}.contamination.table.log"
    shell:
        """
            gatk CalculateContamination  -I {input.tumor} -matched {input.normal} -O {output.table} --tumor-segmentation {output.seg} 2> {log}
        """


# New filtering strategy:
# In order to tweak results in favor of more sensitivity users may set -f-score-beta to a value greater than its default of 1 (beta is the relative weight of sensitivity versus
# precision in the harmonic mean). Setting it lower biases results toward greater precision. (https://gatkforums.broadinstitute.org/gatk/discussion/24057/how-to-call-somatic-mutations-using-gatk4-mutect2#latest)
# -alleles force-call-alleles.vcf
rule filtercalls:
    input: vcf=MUTECT_DIR+"/{sample}.vcf.gz", ref=REF_DIR+"/GRCh38.d1.vd1.fa", stats=MUTECT_DIR+"/{sample}.vcf.gz.stats", contam=MUTECT_DIR+"/{sample}.contamination.table", seg=MUTECT_DIR+"/{sample}.tum.seg", f1r2=MUTECT_DIR+"/{sample}_read-orientation-model.tar.gz"
    output: vcf=MUTECT_DIR+"/{sample}.filtered.vcf.gz", stats=MUTECT_DIR+"/{sample}_filtering_stats.tsv"
    singularity: GATK_SING
    docker: GATK_ODOCKER
    log: MUTECT_DIR+"/{sample}_filtering_stats.tsv.log"
    shell:
        """
            gatk FilterMutectCalls -V {input.vcf} -O {output.vcf} -R {input.ref} --stats {input.stats} --contamination-table {input.contam} --tumor-segmentation={input.seg} --filtering-stats {output.stats} --ob-priors {input.f1r2} 2> {log}
        """


# We remove the POP_AF=1, they are SNP where the reference is mutated, all gnomad has alternate allele, not a somatic mutation
# I think. XXX check if they are still there or not
rule passFilter:
    input: MUTECT_DIR+"/{sample}.filtered.vcf.gz"
    output: MUTECT_DIR+"/{sample}.pass.vcf.gz"
    shell:
        """
        zcat {input} |  awk '/^#/ || $7=="PASS"' | bgzip > {output}
        tabix {output}
        """ 

def get_pos(wildcards):
    if get_ppos is None:
        return 9
    else:
        return get_ppos(wildcards)

rule single_table:
    input: vcf=MUTECT_DIR+"/{sample}.pass.vcf.gz"
    output: MUTECT_DIR+"/{sample}.pass.table.gz"
    params: nsamples=1, pos=get_pos
    shell:
        """
      zcat {input} | grep -v "^##" |  perl -ane '@gt=splice(@F,{params.pos},{params.nsamples}); $gt=""; foreach $g (@gt) {{ if ($.==1) {{$gt.=$g."\\t";}} else {{ @afs = split(":",$g); if ($afs[2] eq ".") {{$afs[2]=0;}} $gt.=$afs[2]."\\t";}} }} chop($gt) ; print $F[2]."\\t".$gt."\\n";' | grep -v "," | gzip > {output}
      """


rule single_table_bis:
    input: vcf=MUTECT_DIR+"/{sample}.pass.vcf.gz"
    output: MUTECT_DIR+"/{sample}.pass.tsv.gz"
    shell:
        """
            bcftools query -s {wildcards.sample} -f '%CHROM:%POS:%REF:%ALT[\\t%SAMPLE\\t%GT\\t%AF]\n' {input.vcf} | gzip > {output}
        """

rule single_table_snv: 
    input: vcf=MUTECT_DIR+"/{sample}.pass.tsv.gz"
    output: MUTECT_DIR+"/{sample}.pass.table.snv.gz"
    shell: 
        """ 
            echo -e "ID\\t{wildcards.sample}" > {output}.tmp
            zcat {input} | cut -f1,4 | tr ":" "\\t" | bawk 'length($3)==1 && length($4==1) {{print".",$5}}' >> {output}.tmp
            gzip -c {output}.tmp > {output}
            rm {output}.tmp
        """

rule neutral_sottoriva:
    input: afmatrix=MUTECT_DIR+"/{sample}.pass.table.snv.gz"
    params: debug=DEBUG, afcolumn="{sample}"
    output: hist="{sample}.hist.{loweraf}_{higheraf}.pdf", fit="{sample}.fit.{loweraf}_{higheraf}.pdf", r2="{sample}.fit.{loweraf}_{higheraf}.r2"
    script: SRC_DIR+"/neutral_sottoriva.R"


# --force-samples had to be added for biobanca early-late due to shared LMH
#bcftools merge --force-samples --missing-to-ref -m none -o {output}.vcf.tmp {input.vcf}
rule merge_targeted:
    input: vcf=expand(MUTECT_DIR+"/{sample}.pass.vcf.gz", sample=TUMOR), bed=SEXONS
    output: MUTECT_DIR+"/merged_targeted.vcf"
    shell:
        """
            bcftools merge --missing-to-ref -m none -o {output}.vcf.tmp {input.vcf}
            bedtools intersect -header -u -a {output}.vcf.tmp -b {input.bed} > {output}
            rm {output}.*.tmp
        """


#rule VEP:
#    input: MUTECT_DIR+"/merged_targeted.vcf"
#    output: txt=MUTECT_DIR+"/merged.vep.txt", html=MUTECT_DIR+"/merged.vep.stats.html", vcf=MUTECT_DIR+"/merged.vcf.id"
#    params: cd=VEP_CACHE_DIR
#    shell:
#        """
#             bcftools annotate -I +'%CHROM:%POS:%REF:%ALT' {input} > {output.vcf}
#             vep -i {output.vcf}  --cache --dir_cache {params.cd} --output_file {output.txt} --stats_file {output.html} --pick
#        """

rule VEP:
    input: MUTECT_DIR+"/merged_targeted.vcf"
    output: vcf=MUTECT_DIR+"/merged.vcf.id"
    params: cd=VEP_CACHE_DIR
    shell:
        """
             bcftools annotate -I +'%CHROM:%POS:%REF:%ALT' {input} > {output.vcf}
        """


rule vcf_to_aftable:
    input: MUTECT_DIR+"/merged.vcf.id"
    output: table=MUTECT_DIR+"/merged.table_nomultiallele"
    params: nsamples=len(SAMPLES), normal="NLH"
    shell:
        """
            cat {input} | grep -v "^##" |  perl -ane '@gt=splice(@F,9); $gt=""; foreach $g (@gt) {{ if ($.==1) {{$gt.=$g."\\t";}} else {{ @afs = split(":",$g); if ($afs[2] eq ".") {{$afs[2]=0;}} $gt.=$afs[2]."\\t";}} }} chop($gt) ; print $F[2]."\\t".$gt."\\n";' | grep -v "," > {output.table}.tmp
            Rscript -e 'd <- read.table("{output.table}.tmp", sep="\\t", header=T); d <- d[, !grepl("{params.normal}",colnames(d))]; write.table(d, file="{output.table}", sep="\\t", quote=F, row.names=F);'
            rm {output.table}.tmp
        """

### annovar
rule merged_bed:
    input: MUTECT_DIR+"/merged.table_nomultiallele"
    output: MUTECT_DIR+"/merged.bed"
    shell:
        """
            sed 1d {input} | tr ":" "\\t" | bawk '{{print $1,$2-1,$2,$3"-"$4}}' > {output}
        """

rule annovar:
    input: MUTECT_DIR+"/merged.bed", ANNOVAR
    output: MUTECT_DIR+"/merged.hg38_multianno.txt"
    log: MUTECT_DIR+"/merged.hg38_multianno.log"
    params: ver="hg38"
    shell:
        """
        sed 's/chr//1;' < {input[0]} | tr "-" "\\t" | bawk '{{if($5=="") {{$5="-"}} if ($4==""){{$4="-"}} b=$2+1; e=b+length($4)-1; print $1,b,e,$4,$5,$6}}' > {output}.tmp
        table_annovar.pl {output}.tmp {input[1]} --otherinfo -buildver {params.ver} -out merged -remove -protocol refGene,avsnp150,cosmic87_coding,nci60,dbnsfp35c,clinvar_20180603 -operation g,f,f,f,f,f -nastring . -polish &> {log}
        rm {output}.tmp
        mv merged.hg38_multianno.txt {output}
        """


# Caution: get entire list of samples, will need to link with QC if we ever remove failed samples, QC has still to be throughly implemented, will never be completely automatic.
# do smt based on all_metrics, also they need to be converted to xlsx (or "align/deDup_{sample}.sorted.bam.flagstat")
rule xlsx_mut:
    input: muts=MUTECT_DIR+'/merged_longformat_{w}tiers.tsv', samples="samples.tsv", genes=GENES
    output: out="{w}_af_table_samples_genes.xlsx"
    params: linkedname=NAME+"_{w}_af_table_samples_genes.xlsx"
    shell:
        """
            bawk '$6!=0' {input.muts} > {output.out}.tmp
            tsv_to_xls -i {output.out}.tmp,{input.samples},{input.genes} -s muts,samples,genes -o {output.out}
            rm -f {params.linkedname}
            ln -s {output.out} {params.linkedname}
            rm {output.out}.tmp
        """

rule samples:
    output: "samples.tsv"
    params: dir=MUTECT_DIR
    shell:
        """
            ls -1 {params.dir}/*pcgr*pass.vcf.gz |  tr "//" "\\t" | tr "." "\\t" | cut -f 2 | sort | uniq > {output} 
        """

## PCGR
rule all_pcgr:
    input: expand(MUTECT_DIR+"/{sample}.pcgr_acmg.grch38.snvs_indels.tiers.tsv", sample=TUMOR)

PCGR='/mnt/trcanmed/snaketree/prj/snakegatk/local/src/pcgr/'
rule pcgr:
    input: MUTECT_DIR+"/{sample}.pass.vcf.gz"
    output: MUTECT_DIR+"/{sample}.pcgr_acmg.grch38.snvs_indels.tiers.tsv"
    params: pcgr=PCGR, size=SIZE, assay=ASSAY, mdir=MUTECT_DIR
    shell:
        """
            python3 {params.pcgr}/pcgr.py --pcgr_dir {params.pcgr} \
            --output_dir {params.mdir} \
            --sample_id {wildcards.sample} \
            --genome_assembly grch38 \
            --conf {params.pcgr}/examples/example_COAD.toml \
            --input_vcf {input} \
            --tumor_site 9 \
            --tumor_purity 1 \
            --include_trials \
            --assay {params.assay} \
            --estimate_msi_status \
            --estimate_tmb \
            --tumor_only \
            --no_vcf_validate \
            --target_size_mb {params.size}
        """

rule pcgr_tiers_af:
    input: MUTECT_DIR+'/merged.table_nomultiallele'
    output: af=MUTECT_DIR+'/merged.table_nomultiallele_wtiers', genes=MUTECT_DIR+'/merged.table_nomultiallele_wtiers_annot', long_muts=MUTECT_DIR+'/merged_longformat_wtiers.tsv'
    params: mdir=MUTECT_DIR, tool=BIN_DIR+'/AFmatrix_filter_pcgr', tiers='TIER 1,TIER 2,TIER 3'
    log: MUTECT_DIR+'/merged.table_nomultiallele_wtiers.log'
    shell:
        """
            {params.tool} -m {input} -o {output.af} -g {output.genes} -w "{params.tiers}" -l {output.long_muts} -u {params.mdir} &> {log}
        """

rule pcgr_tiers_af_all:
    input: MUTECT_DIR+'/merged.table_nomultiallele'
    output: af=MUTECT_DIR+'/merged.table_nomultiallele_alltiers', genes=MUTECT_DIR+'/merged.table_nomultiallele_alltiers_annot', long_muts=MUTECT_DIR+'/merged_longformat_alltiers.tsv'
    params: mdir=MUTECT_DIR, tool=BIN_DIR+'/AFmatrix_filter_pcgr', tiers='TIER 1,TIER 2,TIER 3,TIER 4'
    log: MUTECT_DIR+'/merged.table_nomultiallele_alltiers.log'
    shell:
        """
            {params.tool} -m {input} -o {output.af} -g {output.genes} -w "{params.tiers}" -l {output.long_muts} -u {params.mdir} &> {log}
        """

# For WGS we set SEXONS to all the mappable regions to begin with. Will need more specific rules to compute coverage on regions with at least X coverage (and keep only muts on exons for WES)
rule size:
    input: SEXONS
    output: "targeted_size.txt"
    shell:
        """
            bawk 'BEGIN{{t=0}} {{t=t+($3-$2)}} END {{print t}}' {input} > {output}
        """

rule mut_burden:
    input: tsize="targeted_size.txt", muts=MUTECT_DIR+"/merged.table_nomultiallele"
    output: MUTECT_DIR+"/mut_burden.tsv"
    run:
        import pandas as pd
        import numpy as np
        data = pd.read_csv(input.muts, sep="\t", index_col=0)
        tot = np.count_nonzero(data > 0.1, axis=0)
        target_len = 0
        with open(input.tsize, 'r') as tsize:
            line = tsize.readline()
            target_len = float(line.rstrip())
        target_len = target_len / 1000000
        res = pd.DataFrame(data = {'totmut': tot, 'burden': tot/target_len}, index=data.columns)
        res.to_csv(output[0], sep="\t", index=True)

rule signature:
    input: MUTECT_DIR+"/{sample}.pass.vcf.gz"
    output: "signatures/{sample}.cosmic.fit.tsv"
    params: tool=BIN_DIR+'/mut_pat_signatures_fit_one'
    shell: 
        """
            mkdir -p signatures
            {params.tool} {input} {output} {wildcards.sample}
        """

rule all_signatures:
    input: expand('signatures/{sample}.cosmic.fit.tsv', sample=TUMOR)
    output: 'signatures/all_cosmic_fit.tsv'
    run:    
        import pandas as pd
        res = pd.read_csv(input[0], sep="\t")
        for i in range(1,len(input)):
            resi = pd.read_csv(input[i], sep="\t")
            res = res.merge(resi, left_index=True, right_index=True)
        res.to_csv(output[0], sep="\t")
